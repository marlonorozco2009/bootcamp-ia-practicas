# Importamos las librer√≠as necesarias
from transformers import pipeline
import torch

# NO NECESITAMOS notebook_login() AQU√ç PORQUE YA INICIAMOS SESI√ìN EN LA TERMINAL

# --- 1. CARGA DEL MODELO ---
model_name = "google/gemma-2b"
print(f"Cargando el modelo '{model_name}'...")

try:
    generador = pipeline(
        "text-generation",
        model=model_name,
        device_map="auto",
        torch_dtype=torch.bfloat16
    )
    print("¬°Modelo cargado con √©xito!")

except Exception as e:
    print(f"Error al cargar el modelo: {e}")
    exit()

# --- 2. CREACI√ìN DE LA PLANTILLA DE PROMPT ---
rol = "Eres un guionista de ciencia ficci√≥n experto."
contexto = "Escribe el inicio de una escena para una pel√≠cula donde un robot descubre que puede sentir emociones por primera vez."
restricciones = "La escena debe ser corta, no m√°s de 80 palabras, y terminar con una pregunta del robot a s√≠ mismo."

prompt_completo = f"""
<start_of_turn>user
{rol}

{contexto}

{restricciones}
<end_of_turn>
<start_of_turn>model
ESCENA:
"""

# --- 3. GENERACI√ìN DE LA RESPUESTA ---
print("\n--- Prompt que se enviar√° a la IA ---")
print(prompt_completo)
print("--------------------------------------")
print("\nü§ñ Generando respuesta...")

resultado = generador(
    prompt_completo,
    max_new_tokens=100,
    do_sample=True,
    temperature=0.7,
)

# --- 4. VISUALIZACI√ìN DEL RESULTADO ---
print("\n--- Respuesta de la IA ---")
print(resultado[0]['generated_text'])