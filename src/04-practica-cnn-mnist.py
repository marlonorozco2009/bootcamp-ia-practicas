import torch
import torch.nn as nn
import torch.optim as optim
from torchvision import datasets, transforms
from torch.utils.data import DataLoader

# --- 1. Configuraci√≥n y Par√°metros ---
# Definimos "hiperpar√°metros"
# batch_size: Cu√°ntas im√°genes "estudia" la IA en cada paso
batch_size = 64
# learning_rate: Qu√© tan grandes son los ajustes del optimizador
learning_rate = 0.001
# epochs: Cu√°ntas veces "ver√°" el dataset completo
num_epochs = 3 # 3 es suficiente para una buena demo

# --- 2. Dataset y DataLoader ---
# MNIST es un dataset de n√∫meros escritos a mano (im√°genes de 28x28 p√≠xeles)

# Transformaciones: 
# 1. Convertir la imagen (que es un formato PIL) a un Tensor de PyTorch.
# 2. (ToTensor tambi√©n escala los p√≠xeles de [0, 255] a [0.0, 1.0])
transform = transforms.ToTensor()

# Descargar el "libro de texto" (Dataset) de MNIST
train_dataset = datasets.MNIST(
    root='./data',  # D√≥nde guardar los datos
    train=True,     # Queremos la parte de ENTRENAMIENTO
    transform=transform, 
    download=True   # Desc√°rgalo si no lo tenemos
)
test_dataset = datasets.MNIST(
    root='./data', 
    train=False,    # Queremos la parte de PRUEBA
    transform=transform,
    download=True
)

# DataLoader: El "ayudante" que carga los datos en "batches" (lotes)
# y los "baraja" (shuffle) para que la IA no memorice el orden.
train_loader = DataLoader(dataset=train_dataset, batch_size=batch_size, shuffle=True)
test_loader = DataLoader(dataset=test_dataset, batch_size=batch_size, shuffle=False)

print("‚úÖ Datos de MNIST cargados.")

# --- 3. Modelo (La Red Neuronal Convolucional - CNN) ---
# Esta es la arquitectura de nuestro "cerebro" para ver.

class SimpleCNN(nn.Module):
    def __init__(self):
        super(SimpleCNN, self).__init__()
        
        # --- CAPAS DE "VISI√ìN" (Detectives) ---
        # Conv2d: Es la "linterna" que escanea la imagen 2D buscando patrones
        # (bordes, curvas, etc.).
        # in_channels=1: 1 canal de entrada (la imagen es blanco y negro)
        # out_channels=16: 16 "linternas" (filtros) diferentes buscaremos
        # kernel_size=3: La linterna tiene un tama√±o de 3x3 p√≠xeles
        self.conv1 = nn.Conv2d(in_channels=1, out_channels=16, kernel_size=3, stride=1, padding=1)
        self.relu = nn.ReLU() # Funci√≥n de activaci√≥n
        
        # MaxPool2d: Achica la imagen (de 28x28 a 14x14) para quedarse con lo m√°s importante
        self.pool = nn.MaxPool2d(kernel_size=2, stride=2)
        
        # --- CAPAS DE "DECISI√ìN" (Jueces) ---
        # Flatten: El "aplanador" que convierte la imagen 2D (16x14x14) en un vector 1D
        self.flatten = nn.Flatten()
        
        # Linear: El "juez" que toma el vector 1D (de 16*14*14 = 3136 neuronas)
        # y decide cu√°l de las 10 clases (0-9) es la correcta.
        self.fc = nn.Linear(16 * 14 * 14, 10) 

    def forward(self, x):
        # Este es el "flujo de pensamiento"
        x = self.conv1(x)     # 1. Pasa por la linterna (Conv)
        x = self.relu(x)      # 2. Activa
        x = self.pool(x)      # 3. Achica
        x = self.flatten(x)   # 4. Aplana (de 2D a 1D)
        x = self.fc(x)        # 5. Toma la decisi√≥n final
        return x

model = SimpleCNN()
print(f"‚úÖ Modelo CNN creado. Listo para aprender.")

# --- 4. P√©rdida y Optimizador ---
# (¬°Exactamente igual que el script anterior, pero con una p√©rdida diferente!)

# Funci√≥n de P√©rdida: CrossEntropyLoss es la ideal para clasificaci√≥n
# de M√öLTIPLES categor√≠as (10 n√∫meros), en lugar de solo 2 (BCE).
criterion = nn.CrossEntropyLoss()

# Optimizador: Seguimos usando Adam.
optimizer = optim.Adam(model.parameters(), lr=learning_rate)

# --- 5. Loop de Entrenamiento ---
# (¬°Este flujo es ID√âNTICO al script anterior!)
print(f"üß† Empezando el entrenamiento por {num_epochs} epochs...")

for epoch in range(num_epochs):
    # Iteramos sobre cada "batch" (lote) de im√°genes de entrenamiento
    for i, (images, labels) in enumerate(train_loader):
        # images es un tensor de [64, 1, 28, 28] (64 im√°genes de 1 color de 28x28)
        # labels es un tensor de [64] (las 64 respuestas correctas)
        
        # 1. Forward Pass (Predecir)
        outputs = model(images)
        
        # 2. Calcular la P√©rdida (Error)
        loss = criterion(outputs, labels)
        
        # 3. Backward Pass (Calcular correcci√≥n)
        optimizer.zero_grad()
        loss.backward()
        
        # 4. Actualizar Pesos (Aplicar correcci√≥n)
        optimizer.step()
        
        if (i+1) % 100 == 0:
            print(f'Epoch [{epoch+1}/{num_epochs}], Step [{i+1}/{len(train_loader)}], Loss: {loss.item():.4f}')

print("‚úÖ ¬°Entrenamiento completado!")

# --- 6. Probar el Modelo (El "Examen Final") ---
# Ponemos el modelo en modo "evaluaci√≥n" (desactiva funciones de entrenamiento)
model.eval()

# No necesitamos calcular gradientes (correcciones) durante la prueba
with torch.no_grad():
    correct = 0
    total = 0
    # Iteramos sobre las im√°genes de PRUEBA
    for images, labels in test_loader:
        outputs = model(images)
        # torch.max devuelve el (valor, √≠ndice) de la predicci√≥n m√°s alta.
        # Solo nos importa el √≠ndice (la categor√≠a, el n√∫mero predicho)
        _, predicted = torch.max(outputs.data, 1)
        
        total += labels.size(0) # Sumamos el tama√±o del batch (ej. 64)
        correct += (predicted == labels).sum().item() # Contamos cu√°ntas acert√≥

    accuracy = 100 * correct / total
    print("\n--- Resultados de la Prueba ---")
    print(f"La IA acert√≥ {correct} de {total} im√°genes de prueba.")
    print(f'Precisi√≥n (Accuracy) del modelo en las im√°genes de prueba: {accuracy:.2f} %')